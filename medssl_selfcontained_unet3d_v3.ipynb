{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894388b2",
   "metadata": {},
   "source": [
    "# medssl_selfcontained_unet3d_v3\n",
    "\n",
    "Self‑contained 3D medical image segmentation notebook (U‑Net style), designed to be robust and easy to run.\n",
    "\n",
    "**Highlights**\n",
    "- Clean, end‑to‑end pipeline (I/O → Dataset → Model → Loss/Metric → Train → Eval/Infer)\n",
    "- **Quick Sanity Checks** to run *before* training (tiny overfit test)\n",
    "- Optional AMP + EMA + uncomplicated checkpointing\n",
    "- Hook to load **self‑supervised (SSL) encoder** weights if available\n",
    "\n",
    "> Set your dataset paths in the *Config* cell, then run the *Quick Sanity Checks* section before full training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28f6f4",
   "metadata": {},
   "source": [
    "## 1) Imports & Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, random, time, json, shutil, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Simple 3D augmentations\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False  # better perf\n",
    "    torch.backends.cudnn.benchmark = True       # autotune\n",
    "\n",
    "seed_everything(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b3624",
   "metadata": {},
   "source": [
    "## 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad98b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Paths (EDIT THESE)\n",
    "    data_root: str = \"./data/\"   # root containing volumes/labels\n",
    "    train_list: str = \"./splits/train.txt\"  # each line: <vol_path> <label_path>\n",
    "    val_list: str   = \"./splits/val.txt\"\n",
    "    out_dir: str = \"./runs/medssl_unet3d_v3\"\n",
    "\n",
    "    # I/O\n",
    "    in_channels: int = 1\n",
    "    num_classes: int = 1  # binary by default\n",
    "    patch_size: Tuple[int,int,int] = (96, 96, 96)\n",
    "    norm_min: float = 0.0\n",
    "    norm_max: float = 99.5\n",
    "\n",
    "    # Train\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 2\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    amp: bool = True\n",
    "    use_ema: bool = True\n",
    "    ema_decay: float = 0.999\n",
    "\n",
    "    # Loss\n",
    "    dice_smooth: float = 1.0\n",
    "    bce_weight: float = 0.5  # total = BCE*w + Dice*(1-w)\n",
    "\n",
    "    # Eval\n",
    "    threshold: float = 0.5\n",
    "\n",
    "CFG = Config()\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f6082",
   "metadata": {},
   "source": [
    "## 3) Utilities: I/O, Normalization, Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nibabel as nib  # For NIfTI\n",
    "except Exception:\n",
    "    nib = None\n",
    "\n",
    "def robust_min_max(x: np.ndarray, qmin=0.0, qmax=99.5):\n",
    "    lo, hi = np.percentile(x, [qmin, qmax])\n",
    "    if hi <= lo:\n",
    "        hi = lo + 1e-5\n",
    "    x = np.clip((x - lo) / (hi - lo), 0.0, 1.0)\n",
    "    return x\n",
    "\n",
    "def load_volume(path: str) -> np.ndarray:\n",
    "    if path.lower().endswith(('.nii', '.nii.gz')):\n",
    "        assert nib is not None, \"Install nibabel to read NIfTI.\"\n",
    "        vol = nib.load(path).get_fdata().astype(np.float32)\n",
    "        return vol\n",
    "    elif path.lower().endswith('.npy'):\n",
    "        return np.load(path).astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported volume ext: {path}\")\n",
    "\n",
    "def center_crop_or_pad(vol: np.ndarray, target: Tuple[int,int,int]) -> np.ndarray:\n",
    "    z, y, x = vol.shape[-3:]\n",
    "    tz, ty, tx = target\n",
    "    # Pad\n",
    "    pad_z = max(0, tz - z); pad_y = max(0, ty - y); pad_x = max(0, tx - x)\n",
    "    if pad_z or pad_y or pad_x:\n",
    "        vol = np.pad(vol, ((pad_z//2, pad_z - pad_z//2),\n",
    "                           (pad_y//2, pad_y - pad_y//2),\n",
    "                           (pad_x//2, pad_x - pad_x//2)), mode='edge')\n",
    "        z, y, x = vol.shape\n",
    "    # Center crop\n",
    "    sz = (z - tz)//2; sy = (y - ty)//2; sx = (x - tx)//2\n",
    "    return vol[sz:sz+tz, sy:sy+ty, sx:sx+tx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f8fa8e",
   "metadata": {},
   "source": [
    "## 4) Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolPairDataset(Dataset):\n",
    "    def __init__(self, list_file: str, cfg: Config, augment: bool = False):\n",
    "        self.items = []\n",
    "        with open(list_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                vol_path, lab_path = line.split()\n",
    "                self.items.append((vol_path, lab_path))\n",
    "        self.cfg = cfg\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vpath, lpath = self.items[idx]\n",
    "        vol = load_volume(os.path.join(self.cfg.data_root, vpath))\n",
    "        lab = load_volume(os.path.join(self.cfg.data_root, lpath))\n",
    "\n",
    "        vol = robust_min_max(vol, qmin=self.cfg.norm_min, qmax=self.cfg.norm_max)\n",
    "        lab = (lab > 0).astype(np.float32)  # binary mask\n",
    "\n",
    "        vol = center_crop_or_pad(vol, self.cfg.patch_size)\n",
    "        lab = center_crop_or_pad(lab, self.cfg.patch_size)\n",
    "\n",
    "        # (C, D, H, W)\n",
    "        vol = vol[None, ...]\n",
    "        lab = lab[None, ...]\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                vol = vol[:, ::-1, :, :]\n",
    "                lab = lab[:, ::-1, :, :]\n",
    "            if random.random() < 0.5:\n",
    "                vol = vol[:, :, ::-1, :]\n",
    "                lab = lab[:, :, ::-1, :]\n",
    "            if random.random() < 0.5:\n",
    "                vol = vol[:, :, :, ::-1]\n",
    "                lab = lab[:, :, :, ::-1]\n",
    "\n",
    "        vol = torch.from_numpy(vol.copy())\n",
    "        lab = torch.from_numpy(lab.copy())\n",
    "        return vol, lab\n",
    "\n",
    "def make_loaders(cfg: Config):\n",
    "    train_ds = VolPairDataset(cfg.train_list, cfg, augment=True)\n",
    "    val_ds   = VolPairDataset(cfg.val_list,   cfg, augment=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=1,             shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cf456",
   "metadata": {},
   "source": [
    "## 5) Model: 3D U‑Net (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d26c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, 3, padding=1), nn.InstanceNorm3d(out_ch), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, 3, padding=1), nn.InstanceNorm3d(out_ch), nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_ch=1, n_classes=1, base=16):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock3D(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = ConvBlock3D(base, base*2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.enc3 = ConvBlock3D(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = ConvBlock3D(base*4, base*8)\n",
    "        self.up3 = nn.ConvTranspose3d(base*8, base*4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock3D(base*8, base*4)\n",
    "        self.up2 = nn.ConvTranspose3d(base*4, base*2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock3D(base*4, base*2)\n",
    "        self.up1 = nn.ConvTranspose3d(base*2, base, 2, stride=2)\n",
    "        self.dec1 = ConvBlock3D(base*2, base)\n",
    "        self.outc = nn.Conv3d(base, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b  = self.bottleneck(self.pool3(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        logits = self.outc(d1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34815747",
   "metadata": {},
   "source": [
    "## 6) Losses & Metrics (Dice + BCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        dims = (2,3,4)\n",
    "        intersection = (probs * targets).sum(dim=dims)\n",
    "        union = probs.sum(dim=dims) + targets.sum(dim=dims)\n",
    "        dice = (2*intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss(smooth)\n",
    "        self.w = bce_weight\n",
    "    def forward(self, logits, targets):\n",
    "        return self.w * self.bce(logits, targets) + (1 - self.w) * self.dice(logits, targets)\n",
    "\n",
    "def dice_coefficient(logits, targets, thr=0.5):\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > thr).float()\n",
    "        dims = (2,3,4)\n",
    "        inter = (preds * targets).sum(dim=dims)\n",
    "        union = preds.sum(dim=dims) + targets.sum(dim=dims)\n",
    "        dice = (2*inter) / (union + 1e-8)\n",
    "        return dice.mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfa216",
   "metadata": {},
   "source": [
    "## 7) (Optional) Self‑Supervised Pretraining Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ssl_weights(model: nn.Module, ckpt_path: Optional[str] = None):\n",
    "    if not ckpt_path or not os.path.exists(ckpt_path):\n",
    "        print(\"[SSL] No checkpoint provided — skipping.\")\n",
    "        return\n",
    "    print(f\"[SSL] Loading encoder weights from {ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"[SSL] Missing:\", len(missing), \"Unexpected:\", len(unexpected))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109c8d3",
   "metadata": {},
   "source": [
    "## 8) Trainer with AMP + EMA + Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdf9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.ema = UNet3D(CFG.in_channels, CFG.num_classes)\n",
    "        self.ema.load_state_dict(model.state_dict())\n",
    "        self.decay = decay\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        msd, esd = self.model.state_dict(), self.ema.state_dict()\n",
    "        for k in msd.keys():\n",
    "            esd[k].mul_(self.decay).add_(msd[k], alpha=1 - self.decay)\n",
    "\n",
    "def save_ckpt(model, optimizer, epoch, best_dice, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'best_dice': best_dice,\n",
    "        'cfg': CFG.__dict__\n",
    "    }, path)\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    dices = []\n",
    "    with torch.no_grad():\n",
    "        for vol, lab in loader:\n",
    "            vol = vol.to(device, non_blocking=True)\n",
    "            lab = lab.to(device, non_blocking=True)\n",
    "            logits = model(vol)\n",
    "            dices.append(dice_coefficient(logits, lab, CFG.threshold))\n",
    "    return float(np.mean(dices)) if dices else 0.0\n",
    "\n",
    "def train():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "    train_loader, val_loader = make_loaders(CFG)\n",
    "\n",
    "    model = UNet3D(CFG.in_channels, CFG.num_classes).to(device)\n",
    "    load_ssl_weights(model, ckpt_path=None)  # set SSL path if available\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    loss_fn = BCEDiceLoss(bce_weight=CFG.bce_weight, smooth=CFG.dice_smooth)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.amp)\n",
    "\n",
    "    ema = ModelEMA(model, decay=CFG.ema_decay) if CFG.use_ema else None\n",
    "\n",
    "    best_dice = -1.0\n",
    "    log_path = os.path.join(CFG.out_dir, 'train_log.txt')\n",
    "    with open(log_path, 'w') as f:\n",
    "        f.write('epoch,loss,valDice,best\\n')\n",
    "\n",
    "    for epoch in range(1, CFG.epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n = 0\n",
    "        t0 = time.time()\n",
    "        for vol, lab in train_loader:\n",
    "            vol = vol.to(device, non_blocking=True)\n",
    "            lab = lab.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.amp):\n",
    "                logits = model(vol)\n",
    "                loss = loss_fn(logits, lab)\n",
    "            scaler.scale(loss).backward()\n",
    "            if CFG.grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            if ema: ema.update()\n",
    "            epoch_loss += loss.item() * vol.size(0)\n",
    "            n += vol.size(0)\n",
    "        avg_loss = epoch_loss / max(1, n)\n",
    "\n",
    "        eval_model = ema.ema if ema else model\n",
    "        val_dice = validate(eval_model, val_loader, device)\n",
    "        best_dice = max(best_dice, val_dice)\n",
    "\n",
    "        line = f\"Epoch {epoch:03d} | loss {avg_loss:.4f} | valDice {val_dice:.4f} | best {best_dice:.4f}\"\n",
    "        print(line, f\" | {time.time()-t0:.1f}s\")\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(f\"{epoch},{avg_loss:.6f},{val_dice:.6f},{best_dice:.6f}\\n\")\n",
    "\n",
    "        if (epoch % 5 == 0) or (val_dice >= best_dice):\n",
    "            save_ckpt(eval_model, optimizer, epoch, best_dice, os.path.join(CFG.out_dir, f\"ckpt_e{epoch:03d}.pth\"))\n",
    "\n",
    "    print(\"Training finished. Best Dice:\", best_dice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96630834",
   "metadata": {},
   "source": [
    "## 9) **Quick Sanity Checks** (run *before* training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84564f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def peek_batch(cfg=CFG):\n",
    "    tl, _ = make_loaders(cfg)\n",
    "    vol, lab = next(iter(tl))\n",
    "    print(\"vol shape:\", vol.shape, \"lab shape:\", lab.shape,\n",
    "          \"min/max:\", float(vol.min()), float(vol.max()))\n",
    "    print(\"unique labels:\", torch.unique(lab))\n",
    "\n",
    "def quick_overfit_steps(steps=20):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tl, _ = make_loaders(CFG)\n",
    "    vol, lab = next(iter(tl))\n",
    "    vol, lab = vol.to(device), lab.to(device)\n",
    "\n",
    "    model = UNet3D(CFG.in_channels, CFG.num_classes).to(device)\n",
    "    loss_fn = BCEDiceLoss(bce_weight=CFG.bce_weight, smooth=CFG.dice_smooth)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.amp)\n",
    "\n",
    "    for i in range(1, steps+1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.amp):\n",
    "            logits = model(vol)\n",
    "            loss = loss_fn(logits, lab)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        d = dice_coefficient(logits, lab, thr=CFG.threshold)\n",
    "        print(f\"step {i:02d} loss {loss.item():.4f} dice {d:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1cb84",
   "metadata": {},
   "source": [
    "## 10) Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1) Run sanity checks first:\n",
    "    # peek_batch()\n",
    "    # quick_overfit_steps(steps=30)\n",
    "\n",
    "    # 2) When checks look good, train:\n",
    "    # train()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf5a5e",
   "metadata": {},
   "source": [
    "## 11) Evaluation & Inference Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_volume(model_path: str, vol_path: str, out_path: str = \"./pred.npy\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UNet3D(CFG.in_channels, CFG.num_classes).to(device)\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    if 'model' in state:\n",
    "        model.load_state_dict(state['model'])\n",
    "    else:\n",
    "        model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    vol = load_volume(vol_path)\n",
    "    vol = robust_min_max(vol, qmin=CFG.norm_min, qmax=CFG.norm_max)\n",
    "    vol = center_crop_or_pad(vol, CFG.patch_size)\n",
    "    t = torch.from_numpy(vol[None, None]).float().to(device)\n",
    "    logits = model(t)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0,0]\n",
    "    np.save(out_path, probs)\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d7b133",
   "metadata": {},
   "source": [
    "## 12) Troubleshooting Low Validation Dice\n",
    "- **Data/Label alignment**: confirm voxel spaces & resampling. Misalignment caps Dice ~0.2–0.3.\n",
    "- **Binarization**: ensure labels are 0/1; adjust threshold for class imbalance.\n",
    "- **Normalization**: robust percentile scaling (0–99.5). Try per‑case z‑score if needed.\n",
    "- **Patch size**: increase to (128,128,128) if memory allows.\n",
    "- **Loss mix**: try `bce_weight=0.3` or Focal+Dice for heavy imbalance.\n",
    "- **Augmentations**: start minimal; over‑aggressive aug can hurt.\n",
    "- **Sanity overfit**: must reach ≳0.9 Dice on 1–2 cases.\n",
    "- **Eval**: tune threshold on validation set.\n",
    "- **SSL init**: load encoder weights if available for stability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4971bff5",
   "metadata": {},
   "source": [
    "# medssl_selfcontained_unet3d_v3 ‚Äî **SE‚ÄëResidual U‚ÄëNet++ (Deep Supervision)**\n",
    "\n",
    "Single-file notebook with:\n",
    "- SE‚ÄëResidual U‚ÄëNet++ (3D) **with deep supervision** (default) and a lightweight 3D U‚ÄëNet baseline\n",
    "- **Normalization toggle** (z-score / robust percentiles)\n",
    "- **Cosine LR with warmup**, **gradient accumulation**, **AMP**, optional **EMA**\n",
    "- **Sanity checks** and **validation threshold sweep**\n",
    "\n",
    "Edit the **Config** cell, run sanity checks, then train. üçÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80e4a8",
   "metadata": {},
   "source": [
    "## 1) Imports & Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a28d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, random, time, json, shutil, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import nibabel as nib\n",
    "except Exception:\n",
    "    nib = None\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412131e0",
   "metadata": {},
   "source": [
    "## 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5761e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Paths (EDIT THESE)\n",
    "    data_root: str = \"./data/\"\n",
    "    train_list: str = \"./splits/train.txt\"\n",
    "    val_list: str   = \"./splits/val.txt\"\n",
    "    out_dir: str = \"./runs/medssl_unet3d_v3_sepp\"\n",
    "\n",
    "    # Model & I/O\n",
    "    in_channels: int = 1\n",
    "    num_classes: int = 1\n",
    "    patch_size: Tuple[int,int,int] = (96, 96, 96)\n",
    "    base_channels: int = 24\n",
    "    model_name: str = \"seresunetpp_ds\"  # 'seresunetpp_ds' | 'unet3d_light'\n",
    "    se_reduction: int = 8\n",
    "    dropout: float = 0.0\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 150\n",
    "    batch_size: int = 1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    amp: bool = True\n",
    "    use_ema: bool = False\n",
    "    ema_decay: float = 0.999\n",
    "\n",
    "    # Loss / Deep supervision\n",
    "    dice_smooth: float = 1.0\n",
    "    bce_weight: float = 0.4\n",
    "    ds_weights: Tuple[float, ...] = (0.2, 0.3, 0.5)  # x01, x02, x03\n",
    "\n",
    "    # Eval\n",
    "    threshold: float = 0.5\n",
    "\n",
    "CFG = Config()\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e6a2b-f40a-4847-8e74-f0432f92841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task02_Heart (fold-0) overrides ===\n",
    "CFG.data_root  = \"/path/to/Task02_Heart\"\n",
    "CFG.train_list = \"/path/to/splits/heart_fold0_train.txt\"\n",
    "CFG.val_list   = \"/path/to/splits/heart_fold0_val.txt\"\n",
    "\n",
    "# Model choice & capacity\n",
    "CFG.model_name    = \"seresunetpp_ds\"   # or \"unet3d_light\" for the baseline\n",
    "CFG.base_channels = 24                 # try 32 if VRAM allows\n",
    "\n",
    "# Match nnU-Net plan\n",
    "CFG.patch_size = (80, 192, 160)\n",
    "CFG.batch_size = 1                     # keep memory safe; emulate batch=2 via accumulation\n",
    "\n",
    "# Loss / Deep supervision\n",
    "CFG.bce_weight = 0.4                   # try 0.3 later\n",
    "CFG.ds_weights = (0.2, 0.3, 0.5)\n",
    "\n",
    "# Training schedule (kept from defaults, repeated here for clarity)\n",
    "CFG.epochs        = 150\n",
    "CFG.lr            = 3e-4\n",
    "CFG.weight_decay  = 1e-5\n",
    "CFG.grad_clip     = 1.0\n",
    "CFG.amp           = True\n",
    "CFG.use_ema       = False\n",
    "\n",
    "# Eval\n",
    "CFG.threshold     = 0.5\n",
    "print(\"Overrides applied:\", CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce5889",
   "metadata": {},
   "source": [
    "## 3) Utilities & Normalization Toggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f88525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_min_max(x: np.ndarray, qmin=0.0, qmax=99.5):\n",
    "    lo, hi = np.percentile(x, [qmin, qmax])\n",
    "    if hi <= lo:\n",
    "        hi = lo + 1e-5\n",
    "    x = np.clip((x - lo) / (hi - lo), 0.0, 1.0)\n",
    "    return x\n",
    "\n",
    "def load_volume(path: str) -> np.ndarray:\n",
    "    if path.lower().endswith(('.nii', '.nii.gz')):\n",
    "        assert nib is not None, \"Install nibabel to read NIfTI.\"\n",
    "        vol = nib.load(path).get_fdata().astype(np.float32)\n",
    "        return vol\n",
    "    elif path.lower().endswith('.npy'):\n",
    "        return np.load(path).astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported volume ext: {path}\")\n",
    "\n",
    "def center_crop_or_pad(vol: np.ndarray, target: Tuple[int,int,int]) -> np.ndarray:\n",
    "    z, y, x = vol.shape[-3:]\n",
    "    tz, ty, tx = target\n",
    "    pad_z = max(0, tz - z); pad_y = max(0, ty - y); pad_x = max(0, tx - x)\n",
    "    if pad_z or pad_y or pad_x:\n",
    "        vol = np.pad(vol, ((pad_z//2, pad_z - pad_z//2),\n",
    "                           (pad_y//2, pad_y - pad_y//2),\n",
    "                           (pad_x//2, pad_x - pad_x//2)), mode='edge')\n",
    "        z, y, x = vol.shape\n",
    "    sz = (z - tz)//2; sy = (y - ty)//2; sx = (x - tx)//2\n",
    "    return vol[sz:sz+tz, sy:sy+ty, sx:sx+tx]\n",
    "\n",
    "# === Normalization options ===\n",
    "setattr(CFG, 'norm_mode', 'zscore')           # 'zscore' | 'robust'\n",
    "setattr(CFG, 'zscore_use_nonzero', True)\n",
    "setattr(CFG, 'zscore_clip', 3.0)\n",
    "setattr(CFG, 'norm_min', 0.0)\n",
    "setattr(CFG, 'norm_max', 99.5)\n",
    "\n",
    "def normalize_volume(vol: np.ndarray, cfg=CFG) -> np.ndarray:\n",
    "    mode = getattr(cfg, 'norm_mode', 'zscore')\n",
    "    if mode == 'robust':\n",
    "        return robust_min_max(vol, qmin=getattr(cfg,'norm_min',0.0), qmax=getattr(cfg,'norm_max',99.5)).astype(np.float32)\n",
    "    # z-score (nnU-Net-like)\n",
    "    nz = vol != 0 if getattr(cfg, 'zscore_use_nonzero', True) else np.ones_like(vol, dtype=bool)\n",
    "    if nz.any():\n",
    "        m = vol[nz].mean(); s = vol[nz].std()\n",
    "    else:\n",
    "        m = vol.mean(); s = vol.std()\n",
    "    s = s + 1e-8\n",
    "    vol = (vol - m) / s\n",
    "    clipv = getattr(cfg, 'zscore_clip', 0.0) or 0.0\n",
    "    if clipv and clipv > 0:\n",
    "        vol = np.clip(vol, -clipv, clipv)\n",
    "    return vol.astype(np.float32)\n",
    "\n",
    "print('Normalization mode:', CFG.norm_mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cd7a4",
   "metadata": {},
   "source": [
    "## 4) Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolPairDataset(Dataset):\n",
    "    def __init__(self, list_file: str, cfg: Config, augment: bool = False):\n",
    "        self.items = []\n",
    "        with open(list_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                vol_path, lab_path = line.split()\n",
    "                self.items.append((vol_path, lab_path))\n",
    "        self.cfg = cfg\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vpath, lpath = self.items[idx]\n",
    "        vol = load_volume(os.path.join(self.cfg.data_root, vpath))\n",
    "        lab = load_volume(os.path.join(self.cfg.data_root, lpath))\n",
    "\n",
    "        vol = normalize_volume(vol, self.cfg)\n",
    "        lab = (lab > 0).astype(np.float32)\n",
    "\n",
    "        vol = center_crop_or_pad(vol, self.cfg.patch_size)\n",
    "        lab = center_crop_or_pad(lab, self.cfg.patch_size)\n",
    "\n",
    "        vol = vol[None, ...]\n",
    "        lab = lab[None, ...]\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                vol = vol[:, ::-1, :, :]; lab = lab[:, ::-1, :, :]\n",
    "            if random.random() < 0.5:\n",
    "                vol = vol[:, :, ::-1, :]; lab = lab[:, :, ::-1, :]\n",
    "            if random.random() < 0.5:\n",
    "                vol = vol[:, :, :, ::-1]; lab = lab[:, :, :, ::-1]\n",
    "\n",
    "        vol = torch.from_numpy(vol.copy())\n",
    "        lab = torch.from_numpy(lab.copy())\n",
    "        return vol, lab\n",
    "\n",
    "def make_loaders(cfg: Config):\n",
    "    train_ds = VolPairDataset(cfg.train_list, cfg, augment=True)\n",
    "    val_ds   = VolPairDataset(cfg.val_list,   cfg, augment=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=1,             shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc78e07",
   "metadata": {},
   "source": [
    "## 5) Model Blocks: Residual + SE (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE3D(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 8):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1 = nn.Conv3d(channels, max(1, channels // reduction), kernel_size=1)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv3d(max(1, channels // reduction), channels, kernel_size=1)\n",
    "        self.gate = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x)\n",
    "        w = self.fc2(self.act(self.fc1(w)))\n",
    "        return x * self.gate(w)\n",
    "\n",
    "class ConvResSE3D(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, se_reduction: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm1 = nn.InstanceNorm3d(out_ch)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm3d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.se = SE3D(out_ch, se_reduction)\n",
    "        self.drop = nn.Dropout3d(p=dropout) if dropout > 0 else nn.Identity()\n",
    "        self.skip = nn.Identity() if in_ch == out_ch else nn.Conv3d(in_ch, out_ch, 1)\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        x = self.act(self.norm1(self.conv1(x)))\n",
    "        x = self.norm2(self.conv2(x))\n",
    "        x = x + identity\n",
    "        x = self.se(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec815c",
   "metadata": {},
   "source": [
    "## 6) Lightweight 3D U‚ÄëNet (baseline option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, 3, padding=1), nn.InstanceNorm3d(out_ch), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, 3, padding=1), nn.InstanceNorm3d(out_ch), nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D_Light(nn.Module):\n",
    "    def __init__(self, in_ch=1, n_classes=1, base=16):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock3D(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = ConvBlock3D(base, base*2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.enc3 = ConvBlock3D(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = ConvBlock3D(base*4, base*8)\n",
    "        self.up3 = nn.ConvTranspose3d(base*8, base*4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock3D(base*8, base*4)\n",
    "        self.up2 = nn.ConvTranspose3d(base*4, base*2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock3D(base*4, base*2)\n",
    "        self.up1 = nn.ConvTranspose3d(base*2, base, 2, stride=2)\n",
    "        self.dec1 = ConvBlock3D(base*2, base)\n",
    "        self.outc = nn.Conv3d(base, n_classes, 1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b  = self.bottleneck(self.pool3(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        logits = self.outc(d1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99eee1",
   "metadata": {},
   "source": [
    "## 7) **SE‚ÄëResidual U‚ÄëNet++ (3D) with Deep Supervision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131045ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetPP3D_SERes(nn.Module):\n",
    "    def __init__(self, in_ch: int, n_classes: int, base: int = 24, se_reduction: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        ch = [base, base*2, base*4, base*8]\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.up3_2 = nn.ConvTranspose3d(ch[3], ch[2], 2, stride=2)\n",
    "        self.up2_1 = nn.ConvTranspose3d(ch[2], ch[1], 2, stride=2)\n",
    "        self.up1_0 = nn.ConvTranspose3d(ch[1], ch[0], 2, stride=2)\n",
    "\n",
    "        self.x00 = ConvResSE3D(in_ch, ch[0], se_reduction, dropout)\n",
    "        self.x10 = ConvResSE3D(ch[0], ch[1], se_reduction, dropout)\n",
    "        self.x20 = ConvResSE3D(ch[1], ch[2], se_reduction, dropout)\n",
    "        self.x30 = ConvResSE3D(ch[2], ch[3], se_reduction, dropout)\n",
    "\n",
    "        self.x01 = ConvResSE3D(ch[0] + ch[1], ch[0], se_reduction, dropout)\n",
    "        self.x11 = ConvResSE3D(ch[1] + ch[2], ch[1], se_reduction, dropout)\n",
    "        self.x21 = ConvResSE3D(ch[2] + ch[3], ch[2], se_reduction, dropout)\n",
    "\n",
    "        self.x02 = ConvResSE3D(ch[0] + ch[0] + ch[1], ch[0], se_reduction, dropout)\n",
    "        self.x12 = ConvResSE3D(ch[1] + ch[1] + ch[2], ch[1], se_reduction, dropout)\n",
    "\n",
    "        self.x03 = ConvResSE3D(ch[0] + ch[0] + ch[0] + ch[1], ch[0], se_reduction, dropout)\n",
    "\n",
    "        self.head1 = nn.Conv3d(ch[0], n_classes, 1)\n",
    "        self.head2 = nn.Conv3d(ch[0], n_classes, 1)\n",
    "        self.head3 = nn.Conv3d(ch[0], n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x00 = self.x00(x)\n",
    "        x10 = self.x10(self.pool(x00))\n",
    "        x20 = self.x20(self.pool(x10))\n",
    "        x30 = self.x30(self.pool(x20))\n",
    "\n",
    "        x01 = self.x01(torch.cat([x00, self.up1_0(x10)], dim=1))\n",
    "        x11 = self.x11(torch.cat([x10, self.up2_1(x20)], dim=1))\n",
    "        x21 = self.x21(torch.cat([x20, self.up3_2(x30)], dim=1))\n",
    "\n",
    "        x02 = self.x02(torch.cat([x00, x01, self.up1_0(x11)], dim=1))\n",
    "        x12 = self.x12(torch.cat([x10, x11, self.up2_1(x21)], dim=1))\n",
    "\n",
    "        x03 = self.x03(torch.cat([x00, x01, x02, self.up1_0(x12)], dim=1))\n",
    "\n",
    "        y1 = self.head1(x01)\n",
    "        y2 = self.head2(x02)\n",
    "        y3 = self.head3(x03)\n",
    "        return [y1, y2, y3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca60725",
   "metadata": {},
   "source": [
    "## 8) Build Model Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785172cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg: Config) -> nn.Module:\n",
    "    if cfg.model_name == 'seresunetpp_ds':\n",
    "        return UNetPP3D_SERes(cfg.in_channels, cfg.num_classes, base=cfg.base_channels,\n",
    "                              se_reduction=cfg.se_reduction, dropout=cfg.dropout)\n",
    "    elif cfg.model_name == 'unet3d_light':\n",
    "        return UNet3D_Light(cfg.in_channels, cfg.num_classes, base=cfg.base_channels)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {cfg.model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6dedd",
   "metadata": {},
   "source": [
    "## 9) Losses & Metrics (Dice + BCE) with Deep Supervision Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        dims = (2,3,4)\n",
    "        intersection = (probs * targets).sum(dim=dims)\n",
    "        union = probs.sum(dim=dims) + targets.sum(dim=dims)\n",
    "        dice = (2*intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss(smooth)\n",
    "        self.w = bce_weight\n",
    "    def forward(self, logits, targets):\n",
    "        return self.w * self.bce(logits, targets) + (1 - self.w) * self.dice(logits, targets)\n",
    "\n",
    "def dice_coefficient_from_logits(logits, targets, thr=0.5):\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > thr).float()\n",
    "        dims = (2,3,4)\n",
    "        inter = (preds * targets).sum(dim=dims)\n",
    "        union = preds.sum(dim=dims) + targets.sum(dim=dims)\n",
    "        dice = (2*inter) / (union + 1e-8)\n",
    "        return dice.mean().item()\n",
    "\n",
    "def compute_loss(logits, targets, loss_fn: nn.Module, ds_weights: Tuple[float, ...]):\n",
    "    if isinstance(logits, list):\n",
    "        assert len(logits) == len(ds_weights), f\"DS heads ({len(logits)}) vs weights ({len(ds_weights)})\"\n",
    "        return sum(w * loss_fn(l, targets) for l, w in zip(logits, ds_weights))\n",
    "    else:\n",
    "        return loss_fn(logits, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd35c90",
   "metadata": {},
   "source": [
    "## 10) (Optional) SSL Loader & EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        import copy\n",
    "        self.ema = copy.deepcopy(model)\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.decay = decay\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        msd, esd = self.model.state_dict(), self.ema.state_dict()\n",
    "        for k in msd.keys():\n",
    "            esd[k].mul_(self.decay).add_(msd[k], alpha=1 - self.decay)\n",
    "\n",
    "def load_ssl_weights(model: nn.Module, ckpt_path: Optional[str] = None):\n",
    "    if not ckpt_path or not os.path.exists(ckpt_path):\n",
    "        print(\"[SSL] No checkpoint provided ‚Äî skipping.\")\n",
    "        return\n",
    "    print(f\"[SSL] Loading encoder weights from {ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"[SSL] Missing:\", len(missing), \"Unexpected:\", len(unexpected))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced1954",
   "metadata": {},
   "source": [
    "## 11) Scheduler & Training (cosine warmup + accumulation + AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbc646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, LambdaLR, CosineAnnealingLR\n",
    "\n",
    "# Scheduler extras\n",
    "setattr(CFG, 'warmup_epochs', 5)\n",
    "setattr(CFG, 'min_lr', 1e-6)\n",
    "setattr(CFG, 'accum_steps', 1)\n",
    "\n",
    "def build_scheduler(optimizer, cfg):\n",
    "    warmup_epochs = int(getattr(cfg, 'warmup_epochs', 0))\n",
    "    t_max = max(1, cfg.epochs - warmup_epochs)\n",
    "    schedulers = []\n",
    "    milestones = []\n",
    "    if warmup_epochs > 0:\n",
    "        warmup = LambdaLR(optimizer, lr_lambda=lambda e: (e + 1) / max(1, warmup_epochs))\n",
    "        schedulers.append(warmup)\n",
    "        milestones.append(warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=getattr(cfg, 'min_lr', 0.0))\n",
    "    if milestones:\n",
    "        return SequentialLR(optimizer, schedulers + [cosine], milestones=milestones)\n",
    "    else:\n",
    "        return cosine\n",
    "\n",
    "def validate(model, loader, device, cfg: Config):\n",
    "    model.eval()\n",
    "    dices = []\n",
    "    with torch.no_grad():\n",
    "        for vol, lab in loader:\n",
    "            vol = vol.to(device, non_blocking=True)\n",
    "            lab = lab.to(device, non_blocking=True)\n",
    "            out = model(vol)\n",
    "            logits = out[-1] if isinstance(out, list) else out\n",
    "            dices.append(dice_coefficient_from_logits(logits, lab, cfg.threshold))\n",
    "    return float(np.mean(dices)) if dices else 0.0\n",
    "\n",
    "def train(cfg: Config = CFG):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "    train_loader, val_loader = make_loaders(cfg)\n",
    "\n",
    "    model = build_model(cfg).to(device)\n",
    "    load_ssl_weights(model, ckpt_path=None)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = build_scheduler(optimizer, cfg)\n",
    "    loss_fn = BCEDiceLoss(bce_weight=cfg.bce_weight, smooth=cfg.dice_smooth)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
    "    ema = ModelEMA(model, decay=cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "    best_dice = -1.0\n",
    "    log_path = os.path.join(cfg.out_dir, 'train_log.txt')\n",
    "    with open(log_path, 'w') as f:\n",
    "        f.write('epoch,loss,valDice,best,lr\\n')\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        epoch_loss, n = 0.0, 0\n",
    "        micro = 0\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i, (vol, lab) in enumerate(train_loader, 1):\n",
    "            vol = vol.to(device, non_blocking=True)\n",
    "            lab = lab.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "                out = model(vol)\n",
    "                loss_raw = compute_loss(out, lab, loss_fn, cfg.ds_weights)\n",
    "                loss = loss_raw / max(1, cfg.accum_steps)\n",
    "            scaler.scale(loss).backward()\n",
    "            micro += 1\n",
    "            if micro >= cfg.accum_steps:\n",
    "                if cfg.grad_clip:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if ema: ema.update()\n",
    "                micro = 0\n",
    "            epoch_loss += loss_raw.item() * vol.size(0)\n",
    "            n += vol.size(0)\n",
    "\n",
    "        # Flush remainder\n",
    "        if micro > 0:\n",
    "            if cfg.grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if ema: ema.update()\n",
    "\n",
    "        avg_loss = epoch_loss / max(1, n)\n",
    "        eval_model = ema.ema if ema else model\n",
    "        val_dice = validate(eval_model, val_loader, device, cfg)\n",
    "        best_dice = max(best_dice, val_dice)\n",
    "\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        line = f\"Epoch {epoch:03d} | loss {avg_loss:.4f} | valDice {val_dice:.4f} | best {best_dice:.4f} | lr {lr_now:.2e}\"\n",
    "        print(line, f\"| {time.time()-t0:.1f}s\")\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(f\"{epoch},{avg_loss:.6f},{val_dice:.6f},{best_dice:.6f},{lr_now:.8f}\\n\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Training finished. Best Dice:\", best_dice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bbe4b",
   "metadata": {},
   "source": [
    "## 12) **Quick Sanity Checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def peek_batch(cfg: Config = CFG):\n",
    "    tl, _ = make_loaders(cfg)\n",
    "    vol, lab = next(iter(tl))\n",
    "    print(\"Normalization:\", getattr(cfg, 'norm_mode', 'zscore'))\n",
    "    print(\"vol:\", vol.shape, \"lab:\", lab.shape, \"min/max:\", float(vol.min()), float(vol.max()))\n",
    "    print(\"unique lab values:\", torch.unique(lab))\n",
    "\n",
    "def quick_overfit_steps(steps=20, cfg: Config = CFG):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tl, _ = make_loaders(cfg)\n",
    "    vol, lab = next(iter(tl))\n",
    "    vol, lab = vol.to(device), lab.to(device)\n",
    "\n",
    "    model = build_model(cfg).to(device)\n",
    "    loss_fn = BCEDiceLoss(bce_weight=cfg.bce_weight, smooth=cfg.dice_smooth)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
    "\n",
    "    for i in range(1, steps+1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            out = model(vol)\n",
    "            loss = compute_loss(out, lab, loss_fn, cfg.ds_weights)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        last = out[-1] if isinstance(out, list) else out\n",
    "        d = dice_coefficient_from_logits(last, lab, cfg.threshold)\n",
    "        print(f\"step {i:02d} loss {loss.item():.4f} dice {d:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e83c58",
   "metadata": {},
   "source": [
    "## 13) Validation Threshold Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "@torch.no_grad()\n",
    "def sweep_val_thresholds(model_path: str, thresholds=None, cfg: Config = CFG):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.10, 0.91, 0.05)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = build_model(cfg).to(device)\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    state_dict = state['model'] if isinstance(state, dict) and 'model' in state else state\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    _, val_loader = make_loaders(cfg)\n",
    "\n",
    "    results = []\n",
    "    for thr in thresholds:\n",
    "        dices = []\n",
    "        for vol, lab in val_loader:\n",
    "            vol = vol.to(device, non_blocking=True)\n",
    "            lab = lab.to(device, non_blocking=True)\n",
    "            out = model(vol)\n",
    "            logits = out[-1] if isinstance(out, list) else out\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > float(thr)).float()\n",
    "            dims = (2,3,4)\n",
    "            inter = (preds * lab).sum(dim=dims)\n",
    "            union = preds.sum(dim=dims) + lab.sum(dim=dims)\n",
    "            dice = (2*inter) / (union + 1e-8)\n",
    "            dices.append(dice.mean().item())\n",
    "        mean_dice = float(np.mean(dices)) if dices else 0.0\n",
    "        results.append((float(thr), mean_dice))\n",
    "\n",
    "    best_thr, best_dice = max(results, key=lambda t: t[1]) if results else (0.5, 0.0)\n",
    "    print(\"Threshold sweep results:\")\n",
    "    for thr, sc in results:\n",
    "        print(f\"{thr:.2f}: {sc:.4f}\")\n",
    "    print(f\"Best threshold = {best_thr:.2f}, Dice = {best_dice:.4f}\")\n",
    "    return {'scores': results, 'best': (best_thr, best_dice)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb545f0",
   "metadata": {},
   "source": [
    "## 14) Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e18b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1) Sanity checks\n",
    "    # peek_batch()\n",
    "    # quick_overfit_steps(steps=30)\n",
    "\n",
    "    # 2) Train\n",
    "    # train(CFG)\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
